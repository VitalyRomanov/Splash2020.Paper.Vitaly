@inproceedings{hellendoorn2020global,
  title={Global relational models of source code},
  author={Hellendoorn, Vincent J and Sutton, C and Singh, Rishabh and Maniatis, P},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@article{zhou2018graph,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={arXiv preprint arXiv:1812.08434},
  year={2018}
}



@article{Kanade2019,
abstract = {The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be fine-tuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare fine-tuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when fine-tuned with smaller datasets, and over fewer epochs. We further evaluate CuBERT's effectiveness on a joint classification, localization and repair task involving prediction of two pointers.},
archivePrefix = {arXiv},
arxivId = {2001.00059},
author = {Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
eprint = {2001.00059},
file = {:home/ltv/Downloads/2001.00059.pdf:pdf},
mendeley-groups = {Code Analysis/Pretraining},
pages = {1--22},
title = {{Pre-trained Contextual Embedding of Source Code}},
url = {http://arxiv.org/abs/2001.00059},
year = {2019}
}



@article{Hellendoorn2018,
abstract = {Dynamically typed languages such as JavaScript and Python are increasingly popular, yet static typing has not been totally eclipsed: Python now supports type annotations and languages like Type- Script offer a middle-ground for JavaScript: A strict superset of JavaScript, to which it transpiles, coupled with a type system that permits partially typed programs. However, static typing has a cost: Adding annotations, reading the added syntax, and wrestling with the type system to fix type errors. Type inference can ease the transition to more statically typed code and unlock the benefits of richer compile-time information, but is limited in languages like JavaScript as it cannot soundly handle duck-typing or runtime evaluation via eval. We propose DeepTyper, a deep learning model that understands which types naturally occur in certain contexts and relations and can provide type suggestions, which can often be verified by the type checker, even if it could not infer the type initially. DeepTyper, leverages an automatically aligned corpus of tokens and types to accurately predict thousands of variable and function type annotations. Furthermore, we demonstrate that context is key in accurately assigning these types and introduce a technique to reduce overfitting on local cues while highlighting the need for further improvements. Finally, we show that our model can interact with a compiler to provide more than 4,000 additional type annotations with over 95\% precision that could not be inferred without the aid of DeepTyper.},
annote = {Predicting type annotations for java script. 

Method: Treats program as a sequence of tokens.

Use: Can help programmer to annotate types quicker.},
author = {Hellendoorn, Vincent J. and Bird, Christian and Barr, Earl T. and Allamanis, Miltiadis},
doi = {10.1145/3236024.3236051},
file = {:home/ltv/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hellendoorn et al. - 2018 - Deep learning type inference.pdf:pdf},
isbn = {9781450355735},
journal = {ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
keywords = {Deep Learning,Naturalness,Type Inference},
mendeley-groups = {Code Analysis/Type Inference},
pages = {152--162},
title = {{Deep learning type inference}},
year = {2018}
}
@article{Malik2019,
abstract = {JavaScript is dynamically typed and hence lacks the type safety of statically typed languages, leading to suboptimal IDE support, difficult to understand APIs, and unexpected runtime behavior. Several gradual type systems have been proposed, e.g., Flow and TypeScript, but they rely on developers to annotate code with types. This paper presents NL2Type, a learning-based approach for predicting likely type signatures of JavaScript functions. The key idea is to exploit natural language information in source code, such as comments, function names, and parameter names, a rich source of knowledge that is typically ignored by type inference algorithms. We formulate the problem of predicting types as a classification problem and train a recurrent, LSTM-based neural model that, after learning from an annotated code base, predicts function types for unannotated code. We evaluate the approach with a corpus of 162,673 JavaScript files from real-world projects. NL2Type predicts types with a precision of 84.1\% and a recall of 78.9\% when considering only the top-most suggestion, and with a precision of 95.5\% and a recall of 89.6\% when considering the top-5 suggestions. The approach outperforms both JSNice, a state-of-the-art approach that analyzes implementations of functions instead of natural language information, and DeepTyper, a recent type prediction approach that is also based on deep learning. Beyond predicting types, NL2Type serves as a consistency checker for existing type annotations. We show that it discovers 39 inconsistencies that deserve developer attention (from a manual analysis of 50 warnings), most of which are due to incorrect type annotations.},
author = {Malik, Rabee Sohail and Patra, Jibesh and Pradel, Michael},
doi = {10.1109/ICSE.2019.00045},
file = {:home/ltv/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Malik, Patra, Pradel - 2019 - NL2Type Inferring JavaScript Function Types from Natural Language Information.pdf:pdf},
isbn = {9781728108698},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {JavaScript,comments,deep learning,identifiers,type inference},
mendeley-groups = {Code Analysis/Type Inference},
pages = {304--315},
title = {{NL2Type: Inferring JavaScript Function Types from Natural Language Information}},
volume = {2019-May},
year = {2019}
}


@article{DeFreez2018,
abstract = {Identifying relationships among program elements is useful for program understanding, debugging, and analysis. One such kind of relationship is synonymy. Function synonyms are functions that play a similar role in code; examples include functions that perform initialization for different device drivers, and functions that implement different symmetric-key encryption schemes. Function synonyms are not necessarily semantically equivalent and can be syntactically dissimilar; consequently, approaches for identifying code clones or functional equivalence cannot be used to identify them. This paper presents Func2vec, a technique that learns an embedding mapping each function to a vector in a continuous vector space such that vectors for function synonyms are in close proximity. We compute the function embedding by training a neural network on sentences generated using random walks over the interprocedural control-flow graph. We show the effectiveness of Func2vec at identifying function synonyms in the Linux kernel. Finally, we apply Func2vec to the problem of mining error-handling specifications in Linux file systems and drivers. We show that the function synonyms identified by Func2vec result in error-handling specifications with high support.},
archivePrefix = {arXiv},
arxivId = {1802.07779},
author = {DeFreez, Daniel and Thakur, Aditya V. and Rubio-Gonzalez, Cindy},
doi = {10.1145/3236024.3236059},
eprint = {1802.07779},
file = {:Users/LTV/Downloads/1802.07779.pdf:pdf},
isbn = {9781450355735},
journal = {ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
keywords = {error handling,program analysis,program comprehension,program embeddings,specification mining},
mendeley-groups = {Code Analysis/Similarity Search,Code Analysis/Graphs},
pages = {423--433},
title = {{Path-based function embedding and its application to error-handling specification mining}},
year = {2018}
}

@article{Alon2018a,
abstract = {Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming and increase programmer productivity. A major challenge when learning from programs is $\textit{how to represent programs in a way that facilitates effective learning}$. We present a $\textit{general path-based representation}$ for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens. We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages. We evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both CRF-based and word2vec-based learning, for programs of four languages: JavaScript, Java, Python and C\#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.},
annote = {Mentiones lerning tasks on code.

AST based

Paths in an AST have also been used by Bielik et al. [12] and by Raychev et al. [38, 39] - this is what we are doing

Contributions:
1. New path based representation
2. Cross language prediciton of names

The number of possible paths can be large. Authors approach this by reducing the lenghts of paths, downsampling, etc see 5.6.

Researchers found that limiting the length and width (via fanout of the tree) actually improves the accuracy. They provide the following explanations
- When paths are too long extra information is included in the path. Most of this information can actually be noise, and the most relevant information to determining a variable name is localized in the adjacent lines. 
- Long paths lead to data sparcity

Section 5 tries to answer specific research questions.

Mentiones that to achieve similar result for different languages the sizes of the data had to be disproportionate. The amount of data in general is very large. Interesting to see whether we can achieve similar results by learning representation on graphs, but the datasize is much smaller.

Tasks:
- prediction variable names
- predicting function names
- predicting types in java

They evaluate against bag of symbols as baseline. They say that path is not merely a richer context, because word2vec baseline fails when considers ast path tokens as context. But they did not really consider the number of parameters?

Need to finish reading},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.09544v3},
author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
doi = {10.1145/3192366.3192412},
eprint = {arXiv:1803.09544v3},
file = {:home/ltv/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alon et al. - 2018 - A general path-based representation for predicting program properties.pdf:pdf},
isbn = {9781450356985},
journal = {Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
keywords = {Big code,Learning representations,Machine learning,Programming languages},
mendeley-groups = {Graph as Representation/Code/Other Graph Approaches,Code Analysis/Path-based},
pages = {404--419},
title = {{A general path-based representation for predicting program properties}},
year = {2018}
}



@article{Chen2018,
abstract = {Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.},
annote = {Contribution: tree to tree translation

Graph: AST?

Task: translate java into c{\#}, coffescript to javascript

Approach: using tree-lstm for encoder, tree-lstm decoder},
author = {Chen, Xinyun and Liu, Chang and Song, Dawn},
file = {:Users/LTV/Downloads/7521-tree-to-tree-neural-networks-for-program-translation.pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings},
mendeley-groups = {Code Analysis/Translation,Code Analysis/Graphs},
number = {NeurIPS},
title = {{Tree-to-tree neural networks for program translation}},
year = {2018}
}
@article{Wang2019,
abstract = {Static analysis is an effective technique to catch bugs early when they are easy to fix. Recent advances in program reasoning theory have led to increasing adoption of static analyzers in software engineering practice. Despite the significant progress, there is still potential for improvement. In this paper, we present an alternative approach to create static bug finders. Instead of relying on human expertise, we leverage deep neural networks-which have achieved groundbreaking results in a number of problem domains-to train a static analyzer directly from data. In particular, we frame the problem of bug finding as a classification task and train a classifier to differentiate the buggy from non-buggy programs using Gated Graph Neural Network (GGNN). In addition, we propose a novel interval-based propagation mechanism that significantly improves the generalization of GGNN on larger graphs. We have realized our approach into a framework, NeurSA, and extensively evaluated it. In a cross-project prediction task, three neural bug detectors we instantiate from NeurSA are highly precise in catching null pointer dereference, array index out of bound and class cast bugs in unseen code. A close comparison with Facebook Infer in catching null pointer dereference bugs reveals NeurSA to be far more precise in catching the real bugs and suppressing the spurious warnings. We are in active discussion with Visa Inc for possible adoption of NeurSA in their software development cycle. Due to the effectiveness and generality, we expect NeurSA to be helpful in improving the quality of their code base.},
annote = {Contribution: general tool for intra-procedural analysis for bugs, gnn classify buggy/no buggy; interval based propagation model; 

Task: bug detection

Approach: apparently classify methods into buggy and not buggy, GNN

Graph: control graph. edges have types. nodes from AST; for variables, attach types and super types; do not talk about specific types, but looks very promising},
archivePrefix = {arXiv},
arxivId = {1907.05579},
author = {Wang, Yu and Gao, Fengjuan and Wang, Linzhang and Wang, Ke},
eprint = {1907.05579},
file = {:Users/LTV/Downloads/1907.05579.pdf:pdf},
mendeley-groups = {Code Analysis/Bug fixing,Code Analysis/Graphs},
number = {January},
title = {{Learning a Static Bug Finder from Data}},
url = {http://arxiv.org/abs/1907.05579},
volume = {1},
year = {2019}
}
@article{Concas2007,
abstract = {We present a comprehensive study of an implementation of the Smalltalk object oriented system, one of the first and purest object-oriented programming environment, searching for scaling laws in its properties. We study ten system properties, including the distributions of variable and method names, inheritance hierarchies, class and method sizes, system architecture graph. We systematically found Pareto - or sometimes log-normal - distributions in these properties. This denotes that the programming activity, even when modeled from a statistical perspective, can in no way be simply modeled as a random addition of independent increments with finite variance, but exhibits strong organic dependencies on what has been already developed. We compare our results with similar ones obtained for large Java systems, reported in the literature or computed by ourselves for those properties never studied before, showing that the behavior found is similar in all studied object oriented systems. We show how the Yule process is able to stochastically model the generation of several of the power-laws found, identifying the process parameters and comparing theoretical and empirical tail indexes. Lastly, we discuss how the distributions found are related to existing object-oriented metrics, like Chidamber and Kemerer's, and how they could provide a starting point for measuring the quality of a whole system, versus that of single classes. In fact, the usual evaluation of systems based on mean and standard deviation of metrics can be misleading. It is more interesting to measure differences in the shape and coefficients of the data;s statistical distributions. {\textcopyright} 2007 IEEE.},
annote = {Review of research that discowered power law in source code. Talks about metrics to characterize software. No good metric there is.},
author = {Concas, Giulio and Marchesi, Michele and Pinna, Sandro and Serra, Nicola},
doi = {10.1109/TSE.2007.1019},
file = {:Users/LTV/Downloads/concas2007.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Complexity measures,Object-oriented languages,Object-oriented programming,Product metrics,Software science,Statistical methods,Stochastic processes},
mendeley-groups = {Code Analysis/Graphs},
number = {10},
pages = {687--708},
title = {{Power-laws in a large object-oriented software system}},
volume = {33},
year = {2007}
}
@article{Myers2003,
  title={Software systems as complex networks: Structure, function, and evolvability of software collaboration graphs},
  author={Myers, Christopher R},
  journal={Physical Review E},
  volume={68},
  number={4},
  pages={046116},
  year={2003},
  publisher={APS}
}


@inproceedings{Nguyen2015,
abstract = {n-gram statistical language model has been successfully applied to capture programming patterns to support code completion and suggestion. However, the approaches using n-gram face challenges in capturing the patterns at higher levels of abstraction due to the mismatch between the sequence nature in n-grams and the structure nature of syntax and semantics in source code. This paper presents GraLan, a graph-based statistical language model and its application in code suggestion. GraLan can learn from a source code corpus and compute the appearance probabilities of any graphs given the observed (sub)graphs. We use GraLan to develop an API suggestion engine and an AST-based language model, ASTLan. ASTLan supports the suggestion of the next valid syntactic template and the detection of common syntactic templates. Our empirical evaluation on a large corpus of open-source projects has shown that our engine is more accurate in API code suggestion than the state-of-the-art approaches, and in 75{\%} of the cases, it can correctly suggest the API with only five candidates. ASTLan has also high accuracy in suggesting the next syntactic template and is able to detect many useful and common syntactic templates.},
annote = {Graph based LM. Say they can estimate the probability of a graph given subgraphs.

Graph can capture long range dependencies.
Tested on API recommendation and it worked pretty well.

A Groum is a graph in which
the nodes represent actions (i.e., method calls, overloaded op- erators, and field accesses) and control points (i.e., branching points ofcontrol units if, while, for, etc.). The edges represent the control and data flow dependencies between nodes. The nodes' labels are from the names ofclasses, methods, or control units},
archivePrefix = {arXiv},
arxivId = {quant-ph/0202038},
author = {Nguyen, Anh Tuan and Nguyen, Tien N},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2015.336},
eprint = {0202038},
file = {:Users/LTV/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen, Nguyen - 2015 - Graph-based statistical language model for code.pdf:pdf},
isbn = {9781479919345},
issn = {02705257},
mendeley-groups = {Code Analysis/LM,Code Analysis/Graphs},
pages = {858--868},
pmid = {21700228},
primaryClass = {quant-ph},
title = {{Graph-based statistical language model for code}},
volume = {1},
year = {2015}
}
@article{Ben-Nun2018,
abstract = {With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.},
annote = {Neural Code Comprehension is evaluated on multiple levels, using clustering and analogies for inst2vec, as well as three different code comprehension tasks for XFGs: algorithm classification; heterogeneous compute device (e.g., CPU, GPU) mapping; and optimal thread coarsening factor prediction, which model the runtime of an application without running it.},
archivePrefix = {arXiv},
arxivId = {1806.07336},
author = {Ben-Nun, Tal and Jakobovits, Alice Shoshana and Hoefler, Torsten},
eprint = {1806.07336},
file = {:Users/LTV/Downloads/1806.07336.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Code Analysis/LLVM Representation,Code Analysis/Graphs},
number = {NeurIPS},
pages = {3585--3597},
title = {{Neural code comprehension: A learnable representation of code semantics}},
volume = {2018-Decem},
year = {2018}
}


@inproceedings{Allamanis2017,
  title={Learning to Represent Programs with Graphs},
  author={Allamanis, Miltiadis
          and Brockschmidt, Marc
          and Khademi, Mahmoud},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@article{Fernandes2019,
annote = {Contribution: develop sequence-graph models that are fusion of RNN and GNN for summarization. Have github

Graph: split variable names, each sub token connected with INTOKEN edge, next token edge, ast child, last lexical use

Tasks: Method naming, Docstring generation

Seems to be addressing very similar thing, but still no hierarchy},
archivePrefix = {arXiv},
arxivId = {arXiv:1811.01824v2},
author = {Fernandes, Patrick and Allamanis, Miltiadis and Brockschmidt, Marc and Kingdom, United},
eprint = {arXiv:1811.01824v2},
file = {:Users/LTV/Downloads/structured{\_}neural{\_}summarization.pdf:pdf},
mendeley-groups = {Code Analysis/Summarization,Code Analysis/Graphs,Code Analysis/Name Suggestion},
pages = {1--18},
title = {{Tructured eural ummarization}},
year = {2019}
}
@article{Chae2017,
abstract = {We present a technique for automatically generating features for data-driven program analyses. Recently data-driven approaches for building a program analysis have been proposed, which mine existing codebases and automatically learn heuristics for finding a cost-effective abstraction for a given analysis task. Such approaches reduce the burden of the analysis designers, but they do not remove it completely; they still leave the highly nontrivial task of designing so called features to the hands of the designers. Our technique automates this feature design process. The idea is to use programs as features after reducing and abstracting them. Our technique goes through selected program-query pairs in codebases, and it reduces and abstracts the program in each pair to a few lines of code, while ensuring that the analysis behaves similarly for the original and the new programs with respect to the query. Each reduced program serves as a boolean feature for program-query pairs. This feature evaluates to true for a given program-query pair when (as a program) it is included in the program part of the pair. We have implemented our approach for three real-world program analyses. Our experimental evaluation shows that these analyses with automatically-generated features perform comparably to those with manually crafted features.},
annote = {Contribution: feature extraction for static analysis. the task itself is not clear for me.

Graph: data-flow


do not understand},
archivePrefix = {arXiv},
arxivId = {1612.09394},
author = {Chae, Kwonsoo and Oh, Hakjoo and Heo, Kihong and Yang, Hongseok},
doi = {10.1145/3133925},
eprint = {1612.09394},
file = {:Users/LTV/Downloads/oopsla17.pdf:pdf},
issn = {2475-1421},
journal = {Proceedings of the ACM on Programming Languages},
mendeley-groups = {Code Analysis/Static analysis features,Code Analysis/Graphs},
number = {OOPSLA},
pages = {1--25},
title = {{Automatically generating features for learning program analysis heuristics for C-like languages}},
volume = {1},
year = {2017}
}

@article{Valverde2003,
author = {Valverde, Sergi and Sole, Ricard},
year = {2007},
month = {01},
pages = {1},
title = {Hierarchical Small-Worlds in Software Architecture},
volume = {14},
journal = {Dynamics of Continuous Discrete and Impulsive Systems: Series B; Applications and Algorithms}
}

@article{Parisotto2016,
abstract = {Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.},
annote = {Create a model that synthesizes a program for the task of string processing with regular expression. They use a domain specific language as a target languge. It is likely that DSL has very limited vocabulary, making the search space for a correct progream very small comparing to real programs.

the program represebted as a tree. each symbol in the program is a node and has a representation. each operation is also a node and has a representation. at the first stage, the tree traversed bottom up and relresentation of all notes is computed. representation of operation nodes is computed by applying nn to input arguments. when root reached, root representation is computed. root representation is passed down the tree, and each node acquires a global representation modified by nn.},
archivePrefix = {arXiv},
arxivId = {1611.01855},
author = {Parisotto, Emilio and Mohamed, Abdel-rahman and Singh, Rishabh and Li, Lihong and Zhou, Dengyong and Kohli, Pushmeet},
doi = {10.1063/1.4918698},
eprint = {1611.01855},
file = {:Users/LTV/Library/Application Support/Mendeley Desktop/Downloaded/Parisotto et al. - 2016 - Neuro-Symbolic Program Synthesis.pdf:pdf},
isbn = {0780344782},
issn = {0021-8979},
mendeley-groups = {Code Analysis/Graphs},
pages = {1--14},
title = {{Neuro-Symbolic Program Synthesis}},
url = {http://arxiv.org/abs/1611.01855},
year = {2016}
}



@article{Neubig2014,
abstract = {This paper is an informal survey of methods to generate natural language from source code. 1 Survey Papers Nazara et al. (2015) gives a nice survey on code sum-marization. 2 Generation Methods 2.1 Manual Rules/Templates Perhaps the most common way of creating comment generation systems is through the creation of human rules. The Software Word Usage Model (SWUM) is one of the first models of this type, and can be used for con-verting Java method calls into natural language state-ments (Hill et al., 2009). Sridhara et al. (2010) use the SWUM to create a rule-based model that converts Java to natural-language descriptions. Rule-based approaches have been expanded to cover segments over multiple lines (Sridhara et al., 2011a), full methods (Abid et al., 2015), or full classes (Moreno et al., 2013). There have also been used to cover spe-cial types of code such as test cases (Zhang et al., 2011; Kamimura and Murphy, 2013), code changes (Buse and Weimer, 2010; Cort{\'{e}}s-Coy et al., 2014), or excep-tions (Buse and Weimer, 2008). These methods use a variety of information other than the code itself, including context throught the codebase (McBurney and McMillan, 2014), execution paths (Buse and Weimer, 2008; Zhang et al., 2011), or stereotypical method types (Moreno et al., 2013; Abid et al., 2015). 2.2 Keyword Lists},
author = {Neubig, Graham},
file = {:Users/LTV/Library/Application Support/Mendeley Desktop/Downloaded/Neubig - 2014 - Survey of Methods to Generate Natural Language from Source Code.pdf:pdf},
keywords = {2010a,examine a,haiduc et al,including the lead method,number of methods for,or lsi,scoring,selecting which keywords,summary,tf-idf,there has also been,to use in a,use of topic},
mendeley-groups = {Code Analysis/Generative,Code Analysis/Graphs},
number = {2013},
pages = {2013--2016},
title = {{Survey of Methods to Generate Natural Language from Source Code}},
url = {http://www.languageandcode.org/nlse2015/neubig15nlse-survey.pdf},
year = {2014},
}

@inproceedings{Brockschmidt2018,
  title={Generative Code Modeling with Graphs},
  author={Brockschmidt, Marc
          and Allamanis, Miltiadis
          and Gaunt, Alexander~L. 
          and Polozov, Oleksandr},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

@article{DeMoura2003,
  title={Signatures of small-world and scale-free properties in large computer programs},
  author={De Moura, Alessandro PS and Lai, Ying-Cheng and Motter, Adilson E},
  journal={Physical Review E},
  volume={68},
  number={1},
  pages={017102},
  year={2003},
  publisher={APS}
}

@article{Yau1981,
abstract = {In this paper a hierarchical graph model for programs based on the concepts of recursive graphs (RGâ€˜s) and Codd relations ispresented. The purpose of the model is to clearly represent the structure of a program implemented in a structured language, such as Pascal, Algol, or PL/1, so that the program can be analyzed and modifications to the program can be clearly specified. The model uses an RGrepresentation for the control flow and the data flow with an equivalent relational representation. It also has a relational representation for the objects defined within the program. The various aspects of the model are illustrated using Pascal constructs and a model for an example Pascal program is given. Copyright {\textcopyright} 1981 by The Institute of Electrical and Electronics Engineers, Inc.},
annote = {Contribution: do not understand what they do

Graph: control flow, data flow, 

Approach: ???},
author = {Yau, Stephen S. and Grabow, Paul C.},
doi = {10.1109/TSE.1981.226473},
file = {:Users/LTV/Downloads/yau1981.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Codd relations,control flow,data flow,graph grammar,hierarchical graphs,program analysis,program model,program modifications,program objects,recursive graph},
mendeley-groups = {Graph as Representation/Code,Code Analysis/Unclear goal,Code Analysis/Graphs/Graph Analysis},
number = {6},
pages = {556--574},
title = {{A Model for Representing Programs Using Hierarchical Graphs}},
volume = {SE-7},
year = {1981}
}
@article{Valverde2005,
  title={Network motifs in computational graphs: A case study in software architecture},
  author={Valverde, Sergi and Sol{\'e}, Ricard V},
  journal={Physical Review E},
  volume={72},
  number={2},
  pages={026107},
  year={2005},
  publisher={APS}
}


@inproceedings{
    Alon2018,
    title={code2seq: Generating Sequences from Structured Representations of Code},
    author={Uri Alon and Shaked Brody and Omer Levy and Eran Yahav},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=H1gKYo09tX},
}

@inproceedings{Raychev2015,
  title={Predicting Program Properties from" Big Code"},
  author={Raychev, Veselin and Vechev, Martin and Krause, Andreas},
  booktitle={Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  pages={111--124},
  year={2015}
}

@inproceedings{Cvitkovic2018,
  title={Deep learning on code with an unbounded vocabulary},
  author={Cvitkovic, Milan and Singh, Badal and Anandkumar, Anima},
  booktitle={Machine Learning for Programming (ML4P) Workshop at Federated Logic Conference (FLoC)},
  year={2018}
}

@article{Dinella2020,
abstract = {We present a learning-based approach to detect and fix a broad range of bugs in Javascript programs. We frame the problem in terms of learning a sequence of graph transformations: given a buggy program modeled by a graph structure, our model makes a sequence of predictions including the position of bug nodes and corresponding graph edits to produce a fix. Unlike previous works built upon deep neural networks, our approach targets bugs that are more diverse and complex in nature (i.e. bugs that require adding or deleting statements to fix). We have realized our approach in a tool called HOPPITY. By training on 290,715 Javascript code change commits on Github, HOPPITY correctly detects and fixes bugs in 9,490 out of 36,361 programs in an end-to-end fashion. Given the bug location and type of the fix, HOPPITY also outperforms the baseline approach by a wide margin.},
annote = {Contribution: fixing bugs using a novel graph edit mechanism.

Propose the approach to make program independent of variable names.

Graph: Program is parsed into an abstract syntax three. Links between successive tokens added. Special types of nodes for storing the values are added. They initialize node embedding with the node type. The graph is variable name agnostic since it knows only about the existance of the literal node, not its name. 

The program correction is implemented by generating edit control sequence. The available control statements are given in 3.2.

When predicting edits, they predict the location of the edit usign a pointer network, and then choose the substitution either from a local context or a global set of common variables. The choice is done using attention.

Compare with existing learning approaches for BugRepair and static analysis tools.},
author = {Dinella, Elizabeth and Dai, Hanjun and Brain, Google and Li, Ziyang and Naik, Mayur and Song, Le and Tech, Georgia and Wang, Ke},
file = {:home/ltv/Downloads/iclr20.pdf:pdf},
journal = {Iclr 2020},
mendeley-groups = {Code Analysis/Bug fixing,Code Analysis/Graphs},
pages = {1--17},
title = {{Hoppity: Learning Graph Transformations To Detect and Fix Bugs in Programs}},
year = {2020}
}

@article{Lu2019,
abstract = {Recently, source code mining has received increasing attention due to the rapid increase of open-sourced code repositories and the tremendous values implied in this large dataset, which can help us understand the organization of functions or classes in different software and analyze the impact of these organized patterns on the software behaviors. Hence, learning an effective representation model for the functions of source code, from a modern view, is a crucial problem. Considering the inherent hierarchy of functions, we propose a novel hyperbolic function embedding (HFE) method, which can learn a distributed and hierarchical representation for each function via the Poincar{\'{e}} ball model. To achieve this, a function call graph (FCG) is first constructed to model the call relationship among functions. To verify the underlying geometry of FCG, the Ricci curvature model is used. Finally, an HFE model is built to learn the representations that can capture the latent hierarchy of functions in the hyperbolic space, instead of the Euclidean space, which are usually used in those state-of-the-art methods. Moreover, HFE is more compact in terms of lower dimensionality than the existing graph embedding methods. Thus, HFE is more effective in terms of computation and storage. To experimentally evaluate the performance of HFE, two application scenarios, namely, function classification and link prediction, have been applied. HFE achieves up to 7.6{\%} performance improvement compared to the chosen state-of-the-art methods, namely, Node2vec and Struc2vec.},
annote = {Contribution: hyperbolic embeddings for code

Graph: function call graph

Tools: Doxygen

Approach:

Task: embedding, link prediction},
author = {Lu, Mingming and Liu, Yan and Li, Haifeng and Tan, Dingwu and He, Xiaoxian and Bi, Wenjie and Li, Wendbo},
doi = {10.3390/sym11020254},
file = {:Users/LTV/Library/Application Support/Mendeley Desktop/Downloaded/Lu et al. - 2019 - Hyperbolic function embedding Learning hierarchical representation for functions of source code in hyperbolic space.pdf:pdf},
issn = {20738994},
journal = {Symmetry},
keywords = {Function embedding representation,Function-call graph,Hyperbolic space,Source code mining},
mendeley-groups = {Code Analysis/Topic Modeling,Non-euclidean},
number = {2},
title = {{Hyperbolic function embedding: Learning hierarchical representation for functions of source code in hyperbolic space}},
volume = {11},
year = {2019}
}
@article{Bichsel2016,
abstract = {This work presents a new approach for deobfuscating Android APKs based on probabilistic learning of large code bases (termed "Big Code"). The key idea is to learn a probabilistic model over thousands of non-obfuscated Android applications and to use this probabilistic model to deobfuscate new, unseen Android APKs. The concrete focus of the paper is on reversing layout obfuscation, a popular transformation which renames key program elements such as classes, packages and methods, thus making it difficult to understand what the program does. Concretely, the paper: (i) phrases the layout deobfuscation problem of Android APKs as structured prediction in a probabilistic graphical model, (ii) instantiates this model with a rich set of features and constraints that capture the Android setting, ensuring both semantic equivalence and high prediction accuracy, and (iii) shows how to leverage powerful inference and learning algorithms to achieve overall precision and scalability of the probabilistic predictions. We implemented our approach in a tool called DeGuard and used it to: (i) reverse the layout obfuscation performed by the popular ProGuard system on benign, open-source applications, (ii) predict third-party libraries imported by benign APKs (also obfuscated by ProGuard), and (iii) rename obfuscated program elements of Android malware. The experimental results indicate that DeGuard is practically effective: it recovers 79.1{\%} of the program element names obfuscated with ProGuard, it predicts third-party libraries with accuracy of 91.3{\%}, and it reveals string decoders and classes that handle sensitive data in Android malware.},
annote = {Contribution: probabilistic layout deobfuscation, 

Task: program element naming

Graph: call it dependency graph, similar to inheritance and has edges similar to "defined in". Much simpler, needs a view only of variable and function names.

Approach:

Language: java

Tools:},
author = {Bichsel, Benjamin and Raychev, Veselin and Tsankov, Petar and Vechev, Martin},
doi = {10.1145/2976749.2978422},
file = {:Users/LTV/Library/Application Support/Mendeley Desktop/Downloaded/Bichsel et al. - 2016 - Statistical deobfuscation of Android applications.pdf:pdf},
isbn = {9781450341394},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
mendeley-groups = {Code Analysis/Graphs,Code Analysis/Name Suggestion,Code Analysis/Deobfuscation},
pages = {343--355},
title = {{Statistical deobfuscation of Android applications}},
volume = {24-28-Octo},
year = {2016}
}


@inproceedings{Wang2020,
  title={Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax Tree},
  author={Wang, Wenhan and Li, Ge and Ma, Bo and Xia, Xin and Jin, Zhi},
  booktitle={2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  pages={261--271},
  year={2020},
  organization={IEEE}
}


@article{Drissi2018,
abstract = {The task of translating between programming languages differs from the challenge of translating natural languages in that programming languages are designed with a far more rigid set of structural and grammatical rules. Previous work has used a tree-to-tree encoder/decoder model to take advantage of the inherent tree structure of programs during translation. Neural decoders, however, by default do not exploit known grammar rules of the target language. In this paper, we describe a tree decoder that leverages knowledge of a language's grammar rules to exclusively generate syntactically correct programs. We find that this grammar-based tree-to-tree model outperforms the state of the art tree-to-tree model in translating between two programming languages on a previously used synthetic task.},
annote = {Contribution: 

Graph: AST?

Task: translation},
archivePrefix = {arXiv},
arxivId = {1807.01784},
author = {Drissi, Mehdi and Watkins, Olivia and Khant, Aditya and Ojha, Vivaswat and Sandoval, Pedro and Segev, Rakia and Weiner, Eric and Keller, Robert},
eprint = {1807.01784},
file = {:Users/LTV/Downloads/1807.01784.pdf:pdf},
mendeley-groups = {Code Analysis/Translation},
title = {{Program Language Translation Using a Grammar-Driven Tree-to-Tree Model}},
url = {http://arxiv.org/abs/1807.01784},
volume = {2},
year = {2018}
}
@article{Xu2017,
abstract = {The problem of cross-platform binary code similarity detection aims at detecting whether two binary functions coming from different platforms are similar or not. It has many security applications, including plagiarism detection, malware detection, vulnerability search, etc. Existing approaches rely on approximate graphmatching algorithms, which are inevitably slow and sometimes inaccurate, and hard to adapt to a new task. To address these issues, in this work, we propose a novel neural network-based approach to compute the embedding, i.e., a numeric vector, based on the control flow graph of each binary function, then the similarity detection can be done efficiently by measuring the distance between the embeddings for two functions. We implement a prototype called Gemini. Our extensive evaluation shows that Gemini outperforms the state-of-the-art approaches by large margins with respect to similarity detection accuracy. Further, Gemini can speed up prior art's embedding generation time by 3 to 4 orders of magnitude and reduce the required training time from more than 1 week down to 30 minutes to 10 hours. Our real world case studies demonstrate that Gemini can identify significantly more vulnerable firmware images than the state-of-the-art, i.e., Genius. Our research showcases a successful application of deep learning on computer security problems.},
annote = {Contribution: graph based approach to similarity detecion in cross-platform binary code

Graph: control flow, node attributes

Task: similarity search, graph embedding},
archivePrefix = {arXiv},
arxivId = {1708.06525},
author = {Xu, Xiaojun and Liu, Chang and Feng, Qian and Yin, Heng and Song, Le and Song, Dawn},
doi = {10.1145/3133956.3134018},
eprint = {1708.06525},
file = {:Users/LTV/Downloads/3133956.3134018.pdf:pdf},
isbn = {9781450349468},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
keywords = {Binary Code,Neural Network,Similarity Detection},
mendeley-groups = {Code Analysis/Similarity Search,Code Analysis/Graphs},
pages = {363--376},
title = {{Neural network-based graph embedding for cross-platform binary code similarity detection}},
year = {2017}
}
@article{VietPhan2018,
abstract = {Existing defects in software components is unavoidable and leads to not only a waste of time and money but also many serious consequences. To build predictive models, previous studies focus on manually extracting features or using tree representations of programs, and exploiting different machine learning algorithms. However, the performance of the models is not high since the existing features and tree structures often fail to capture the semantics of programs. To explore deeply programs' semantics, this paper proposes to leverage precise graphs representing program execution flows, and deep neural networks for automatically learning defect features. Firstly, control flow graphs are constructed from the assembly instructions obtained by compiling source code; we thereafter apply multi-view multi-layer directed graph-based convolutional neural networks (DGCNNs) to learn semantic features. The experiments on four real-world datasets show that our method significantly outperforms the baselines including several other deep learning approaches.},
annote = {Contribution: application of control flow and GNN, control graph of assembly code

Graph: control flow

Task: predict type of defect},
archivePrefix = {arXiv},
arxivId = {1802.04986},
author = {{Viet Phan}, Anh and {Le Nguyen}, Minh and {Thu Bui}, Lam},
doi = {10.1109/ICTAI.2017.00019},
eprint = {1802.04986},
file = {:Users/LTV/Downloads/1802.04986.pdf:pdf},
isbn = {9781538638767},
issn = {10823409},
journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
keywords = {Control-Flow-Graphs,Convolutional-Neural-Networks,Software-Defect-Prediction},
mendeley-groups = {Code Analysis/Classification},
pages = {45--52},
title = {{Convolutional neural networks over control flow graphs for software defect prediction}},
volume = {2017-Novem},
year = {2018}
}
@article{Shido2019,
abstract = {Neural machine translation models are used to automatically generate a document from given source code since this can be regarded as a machine translation task. Source code summarization is one of the components for automatic document generation, which generates a summary in natural language from given source code. This suggests that techniques used in neural machine translation, such as Long Short-Term Memory (LSTM), can be used for source code summarization. However, there is a considerable difference between source code and natural language: Source code is essentially structured, having loops and conditional branching, etc. Therefore, there is some obstacle to apply known machine translation models to source code.Abstract syntax trees (ASTs) capture these structural properties and play an important role in recent machine learning studies on source code. Tree-LSTM is proposed as a generalization of LSTMs for tree-structured data. However, there is a critical issue when applying it to ASTs: It cannot handle a tree that contains nodes having an arbitrary number of children and their order simultaneously, which ASTs generally have such nodes. To address this issue, we propose an extension of Tree-LSTM, which we call Multi-way Tree-LSTM and apply it for source code summarization. As a result of computational experiments, our proposal achieved better results when compared with several state-of-the-art techniques.},
annote = {Contribution: tree LSTM extension to support AST of different sizes

Graph: AST

Task: summarization (propper text generation)},
archivePrefix = {arXiv},
arxivId = {1906.08094},
author = {Shido, Yusuke and Kobayashi, Yasuaki and Yamamoto, Akihiro and Miyamoto, Atsushi and Matsumura, Tadayuki},
doi = {10.1109/IJCNN.2019.8851751},
eprint = {1906.08094},
file = {:Users/LTV/Downloads/1906.08094.pdf:pdf},
isbn = {9781728119854},
journal = {Proceedings of the International Joint Conference on Neural Networks},
mendeley-groups = {Code Analysis/Summarization},
pages = {1--14},
title = {{Automatic Source Code Summarization with Extended Tree-LSTM}},
volume = {2019-July},
year = {2019}
}
@article{Wan2019,
abstract = {Code retrieval techniques and tools have been playing a key role in facilitating software developers to retrieve existing code fragments from available open-source repositories given a user query (e.g., a short natural language text describing the functionality for retrieving a particular code snippet). Despite the existing efforts in improving the effectiveness of code retrieval, there are still two main issues hindering them from being used to accurately retrieve satisfiable code fragments from large-scale repositories when answering complicated queries. First, the existing approaches only consider shallow features of source code such as method names and code tokens, but ignoring structured features such as abstract syntax trees (ASTs) and control-flow graphs (CFGs) of source code, which contains rich and well-defined semantics of source code. Second, although the deep learning-based approach performs well on the representation of source code, it lacks the explainability, making it hard to interpret the retrieval results and almost impossible to understand which features of source code contribute more to the final results. To tackle the two aforementioned issues, this paper proposes MMAN, a novel Multi-Modal Attention Network for semantic source code retrieval. A comprehensive multi-modal representation is developed for representing unstructured and structured features of source code, with one LSTM for the sequential tokens of code, a Tree-LSTM for the AST of code and a GGNN (Gated Graph Neural Network) for the CFG of code. Furthermore, a multi-modal attention fusion layer is applied to assign weights to different parts of each modality of source code and then integrate them into a single hybrid representation. Comprehensive experiments and analysis on a large-scale real-world dataset show that our proposed model can accurately retrieve code snippets and outperforms the state-of-the-art methods.},
annote = {Contribution: multimodal view through AST and CFG, modality attention

Graph: AST, CFG

Task: search},
archivePrefix = {arXiv},
arxivId = {1909.13516},
author = {Wan, Yao and Shu, Jingdong and Sui, Yulei and Xu, Guandong and Zhao, Zhou and Wu, Jian and Yu, Philip},
doi = {10.1109/ASE.2019.00012},
eprint = {1909.13516},
file = {:Users/LTV/Downloads/1909.13516.pdf:pdf},
isbn = {9781728125084},
journal = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
keywords = {Attention Mechanism,Code retrieval,Deep learning,Multi-modal network},
mendeley-groups = {Code Analysis/Similarity Search,Code Analysis/Graphs},
pages = {13--25},
title = {{Multi-modal attention network learning for semantic source code retrieval}},
year = {2019}
}
@article{Lacomis2019,
abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3{\%} of the time.},
archivePrefix = {arXiv},
arxivId = {1909.09029},
author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward and Allamanis, Miltiadis and {Le Goues}, Claire and Neubig, Graham and Vasilescu, Bogdan},
doi = {10.1109/ASE.2019.00064},
eprint = {1909.09029},
file = {:Users/LTV/Downloads/1909.09029.pdf:pdf},
isbn = {9781728125084},
journal = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
keywords = {Decompilation,Deep learning},
mendeley-groups = {Code Analysis/Name Suggestion,Code Analysis/Graphs},
pages = {628--639},
title = {{DIRE: A neural approach to decompiled identifier naming}},
year = {2019}
}
@article{Brauckmann2020,
annote = {Contribution: used DL for compilers

Task: decite whether OpenCL kernel should execute on CPU or GPU

Graph: AST, Control-data-flow

Approach: GNN},
author = {Brauckmann, Alexander and Goens, Andr{\'{e}}s and Ertel, Sebastian and Castrillon, Jeronimo},
doi = {10.1145/3377555.3377894},
file = {:Users/LTV/Downloads/3377555.3377894.pdf:pdf},
isbn = {9781450371209},
journal = {Proceedings of the 29th International Conference on Compiler Construction},
keywords = {all or part of,are not made or,commercial advantage and that,compilers,deep learning,distributed for profit or,graphs,is granted without fee,llvm,or hard copies of,permission to make digital,personal or classroom use,provided that copies,this work for},
mendeley-groups = {Code Analysis/Classification,Code Analysis/Graphs},
pages = {201--211},
title = {{Compiler-based graph representations for deep learning models of code}},
url = {https://dl.acm.org/doi/10.1145/3377555.3377894},
year = {2020}
}

@article{Alon2019,
  title={Structural Language Models for Any-Code Generation},
  author={Alon, Uri and Sadaka, Roy and Levy, Omer and Yahav, Eran},
  journal={arXiv preprint arXiv:1910.00577},
  year={2019}
}

@article{Yang2019,
abstract = {In last few years, using a language model such as LSTM to train code token sequences is the state-of-art to get a code generation model. However, source code can be viewed not only as a token sequence but also as a syntax tree. Treating all source code tokens equally will lose valuable structural information. Recently, in code synthesis tasks, tree models such as Seq2Tree and Tree2Tree have been proposed to generate code and those models perform better than LSTM-based seq2seq methods. In those models, encoding model encodes user-provided information such as the description of the code, and decoding model decodes code based on the encoding results of user-provided information. When applying decoding model to decode code, current models pay little attention to the context of the already decoded code. According to experiments, using tree models to encode the already decoded code and predicting next code based on tree representations of the already decoded code can improve the decoding performance. Thus, in this paper, we propose a novel tree language model (TLM) which predicts code based on a novel tree encoding of the already decoded code (context). The experiments indicate that the proposed method outperforms state-of-arts in code completion.},
annote = {Contribution: predict next code based on accumulated representations of subtrees, two dimensional LSTM

Graph: AST

Task: LM, autocompletion},
author = {Yang, Yixiao and Chen, Xiang and Sun, Jiaguang},
doi = {10.18293/SEKE2019-057},
file = {:Users/LTV/Downloads/seke19paper{\_}57.pdf:pdf},
isbn = {1891706489},
issn = {23259086},
journal = {Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
keywords = {Code completion,Tree encoding of context,Tree language model},
mendeley-groups = {Code Analysis/LM},
pages = {675--680},
title = {{Improve language modelling for code completion by tree language model with tree encoding of context}},
volume = {2019-July},
year = {2019}
}
@article{Zhang2019,
abstract = {Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classification and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.},
annote = {Contribution: improvement over tree-lstm by adding control flow

Graph: AST fragments?, control flow

Task: classification, clone detection

Approach: split AST into small ASTs and then encode earch and apply LSTM??},
author = {Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Wang, Kaixuan and Liu, Xudong},
doi = {10.1109/ICSE.2019.00086},
file = {:Users/LTV/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2019 - A Novel Neural Source Code Representation Based on Abstract Syntax Tree.pdf:pdf},
isbn = {9781728108698},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Abstract Syntax Tree,code classification,code clone detection,neural network,source code representation},
mendeley-groups = {Code Analysis/Classification,Code Analysis/Similarity Search,Code Analysis/Graphs},
pages = {783--794},
title = {{A Novel Neural Source Code Representation Based on Abstract Syntax Tree}},
volume = {2019-May},
year = {2019}
}

@inproceedings{Zhou2019,
  title={Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks},
  author={Zhou, Yaqin and Liu, Shangqing and Siow, Jingkai and Du, Xiaoning and Liu, Yang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10197--10207},
  year={2019}
}

@article{Hung2019,
abstract = {In recent years, malware has grown constantly in both quantity and complexity. Traditional malware detection methods such as string search, hash code comparison, etc. have to face the challenging appearance of more and more new malware variations. One of the most promising approaches to tackling them is to use machine learning techniques to automatically analyze and detect unknown malicious softwares. In this paper, we introduce a novel method of using dynamic behavior data to represent malicious code in the form of multi-edge directed quantitative data flow graphs and a deep learning technique to detect malicious code. Our experimental result shows that the proposed method archived a higher detection rate than other machine learning methods, and a higher unknown malware detection rate, compared with commercial antivirus software.},
author = {Hung, Nguyen Viet and {Ngoc Dung}, Pham and Ngoc, Tran Nguyen and {Dinh Phai}, Vu and Shi, Qi},
doi = {10.1109/KSE.2019.8919284},
file = {:Users/LTV/Library/Application Support/Mendeley Desktop/Downloaded/Hung et al. - 2019 - Malware detection based on directed multi-edge dataflow graph representation and convolutional neural network.pdf:pdf},
isbn = {9781728130033},
journal = {Proceedings of 2019 11th International Conference on Knowledge and Systems Engineering, KSE 2019},
keywords = {Graph convolutional neural network,Malicious software,Malware dynamic analysis,Malware representation},
mendeley-groups = {Code Analysis/Classification},
pages = {1--5},
publisher = {IEEE},
title = {{Malware detection based on directed multi-edge dataflow graph representation and convolutional neural network}},
year = {2019}
}
@article{Dam2019,
abstract = {Defects are common in software systems and cause many problems for software users. Different methods have been developed to make early prediction about the most likely defective modules in large codebases. Most focus on designing features (e.g. complexity metrics) that correlate with potentially defective code. Those approaches however do not sufficiently capture the syntax and multiple levels of semantics of source code, a potentially important capability for building accurate prediction models. In this paper, we report on our experience of deploying a new deep learning tree-based defect prediction model in practice. This model is built upon the tree-structured Long Short Term Memory network which directly matches with the Abstract Syntax Tree representation of source code. We discuss a number of lessons learned from developing the model and evaluating it on two datasets, one from open source projects contributed by our industry partner Samsung and the other from the public PROMISE repository.},
annote = {Contribution: see lessons

Task: defect classification

Graph: AST

Approach: Tree-LSTM},
author = {Dam, Hoa Khanh and Pham, Trang and Ng, Shien Wee and Tran, Truyen and Grundy, John and Ghose, Aditya and Kim, Taeksu and Kim, Chul Joo},
doi = {10.1109/MSR.2019.00017},
file = {:Users/LTV/Library/Application Support/Mendeley Desktop/Downloaded/Dam et al. - 2019 - Lessons learned from using a deep tree-based model for software defect prediction in practice.pdf:pdf},
isbn = {9781728134123},
issn = {21601860},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {Deep learning,Defect prediction},
mendeley-groups = {Code Analysis/Classification,Code Analysis/Graphs},
pages = {46--57},
title = {{Lessons learned from using a deep tree-based model for software defect prediction in practice}},
volume = {2019-May},
year = {2019}
}

@article{Yamaguchi2014,
abstract = {The vast majority of security breaches encountered today are a direct result of insecure code. Consequently, the protection of computer systems critically depends on the rigorous identification of vulnerabilities in software, a tedious and error-prone process requiring significant expertise. Unfortunately, a single flaw suffices to undermine the security of a system and thus the sheer amount of code to audit plays into the attacker's cards. In this paper, we present a method to effectively mine large amounts of source code for vulnerabilities. To this end, we introduce a novel representation of source code called a code property graph that merges concepts of classic program analysis, namely abstract syntax trees, control flow graphs and program dependence graphs, into a joint data structure. This comprehensive representation enables us to elegantly model templates for common vulnerabilities with graph traversals that, for instance, can identify buffer overflows, integer overflows, format string vulnerabilities, or memory disclosures. We implement our approach using a popular graph database and demonstrate its efficacy by identifying 18 previously unknown vulnerabilities in the source code of the Linux kernel.},
annote = {Contribution: unite all the graphs into one for vulnerability detection.

Graph: AST, control flow, data dependence,

Language:

Task: vulnerability detection

Appears no learning},
author = {Yamaguchi, Fabian and Golde, Nico and Arp, Daniel and Rieck, Konrad},
doi = {10.1109/SP.2014.44},
file = {:Users/LTV/Downloads/06956589.pdf:pdf},
isbn = {9781479946860},
issn = {10816011},
journal = {Proceedings - IEEE Symposium on Security and Privacy},
keywords = {Graph Databases,Static Analysis,Vulnerabilities},
mendeley-groups = {Code Analysis/Vulnerability detection,Code Analysis/Graphs},
pages = {590--604},
publisher = {IEEE},
title = {{Modeling and discovering vulnerabilities with code property graphs}},
year = {2014}
}
@article{Patterson2018,
abstract = {Your computer is continuously executing programs, but does it really understand them? Not in any meaningful sense. That burden falls upon human knowledge workers, who are increasingly asked to write and understand code. They deserve to have intelligent tools that reveal the connections between code and its subject matter. Towards this prospect, we develop an AI system that forms semantic representations of computer programs, using techniques from knowledge representation and program analysis. To create the representations, we introduce an algorithm for enriching dataflow graphs with semantic information. The semantic enrichment algorithm is undergirded by a new ontology language for modeling computer programs and a new ontology about data science, written in this language. Throughout the paper, we focus on code written by data scientists and we locate our work within a larger movement towards collaborative, open, and reproducible science.},
annote = {Contribution: converting program into semantic flow graph. I.e. making program independent from libraries, describing functionality.

Graph: ???},
archivePrefix = {arXiv},
arxivId = {1807.05691},
author = {Patterson, Evan and Baldini, Ioana and Mojsilovic, Aleksandra and Varshney, Kush R.},
eprint = {1807.05691},
file = {:Users/LTV/Downloads/2018-semantic-enrichment.pdf:pdf},
mendeley-groups = {Code Analysis/Summarization},
title = {{Teaching machines to understand data science code by semantic enrichment of dataflow graphs}},
url = {http://arxiv.org/abs/1807.05691},
year = {2018}
}

@article{Buch2019,
abstract = {Code clone detection remains a crucial challenge in maintaining software projects. Many classic approaches rely on handcrafted aggregation schemes, while recent work uses supervised or unsupervised learning. In this work, we study several aspects of aggregation schemes for code clone detection based on supervised learning. To this aim, we implement an AST-based Recursive Neural Network. Firstly, our ablation study shows the influence of model choices and hyperparameters. We introduce error scaling as a way to effectively and efficiently address the class imbalance problem arising in code clone detection. Secondly, we study the influence of pretrained embeddings representing nodes in ASTs. We show that simply averaging all node vectors of a given AST yields strong baseline aggregation scheme. Further, learned AST aggregation schemes greatly benefit from pretrained node embeddings. Finally, we show the importance of carefully separating training and test data by clone clusters, to reliably measure generalization of models learned with supervision.},
annote = {Contribution: AST RNN for representing functions and clone detection

Graph: AST

Task: clone datection

Approach: Recursive Neural Network processes input as a tree

AST appears to be a baseline....?},
author = {Buch, Lutz and Andrzejak, Artur},
doi = {10.1109/SANER.2019.8668039},
file = {:Users/LTV/Downloads/Buech-Andrzejak-SANER2019.pdf:pdf},
isbn = {9781728105918},
journal = {SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering},
keywords = {Abstract Syntax Trees,Code Clone Detection,Embeddings,Recursive Neural Network,Siamese Network},
mendeley-groups = {Code Analysis/Similarity Search},
pages = {95--104},
title = {{Learning-Based Recursive Aggregation of Abstract Syntax Trees for Code Clone Detection}},
year = {2019}
}
@article{Tufano2018,
abstract = {Assessing the similarity between code components plays a pivotal role in a number of Software Engineering (SE) tasks, such as clone detection, impact analysis, refactoring, etc. Code similarity is generally measured by relying on manually defined or hand-crafted features, e.g., by analyzing the overlap among identifiers or comparing the Abstract Syntax Trees of two code components. These features represent a best guess at what SE researchers can utilize to exploit and reliably assess code similarity for a given task. Recent work has shown, when using a stream of identifiers to represent the code, that Deep Learning (DL) can effectively replace manual feature engineering for the task of clone detection. However, source code can be represented at different levels of abstraction: identifiers, Abstract Syntax Trees, Control Flow Graphs, and Bytecode. We conjecture that each code representation can provide a different, yet orthogonal view of the same code fragment, thus, enabling a more reliable detection of similarities in code. In this paper, we demonstrate how SE tasks can benefit from a DL-based approach, which can automatically learn code similarities from different representations.},
annote = {Studies how other people represent source code for DL

Mentiones a lot of old papers. No new papers for Graph preresentation found here},
author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and {Di Penta}, Massimiliano and White, Martin and Poshyvanyk, Denys},
doi = {10.1145/3196398.3196431},
file = {:Users/LTV/Downloads/MSR'18-DeepRepresentations-CRC.pdf:pdf},
isbn = {9781450357166},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {code similarities,deep learning,neural networks},
mendeley-groups = {Code Analysis/Survey},
pages = {542--553},
title = {{Deep learning similarities from different representations of source code}},
year = {2018}
}

@article{Li2019,
  title={Improving bug detection via context-based code representation learning and attention-based neural networks},
  author={Li, Yi and Wang, Shaohua and Nguyen, Tien N and Van Nguyen, Son},
  journal={Proceedings of the ACM on Programming Languages},
  volume={3},
  number={OOPSLA},
  pages={1--30},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{Chakraborty2018,
abstract = {The way developers edit day-to-day code tend to be repetitive and often use existing code elements. Many researchers tried to automate this tedious task of code changes by learning from specific change templates and applied to limited scope. The advancement of Neural Machine Translation (NMT) and the availability of the vast open source software evolutionary data open up a new possibility of automatically learning those templates from the wild. However, unlike natural languages, for which NMT techniques were originally designed, source code and the changes have certain properties. For instance, compared to natural language source code vocabulary can be virtually infinite. Further, any good change in code should not break its syntactic structure. Thus, deploying state-of-the-art NMT models without domain adaptation may poorly serve the purpose. To this end, in this work, we propose a novel Tree2Tree Neural Machine Translation system to model source code changes and learn code change patterns from the wild. We realize our model with a change suggestion engine: CODIT. We train the model with more than 30k real-world changes and evaluate it with 6k patches. Our evaluation shows the effectiveness of CODIT in learning and suggesting abstract change templates. CODIT also shows promise in suggesting concrete patches and generating bug fixes.},
annote = {Contribution:

Task: translation

Graph: AST

Approach: use BiGRU for encoding leafs anf then use tree structure to compute non-terminal nodes},
archivePrefix = {arXiv},
arxivId = {1810.00314},
author = {Chakraborty, Saikat and Allamanis, Miltiadis and Ray, Baishakhi},
eprint = {1810.00314},
file = {:Users/LTV/Library/Application Support/Mendeley Desktop/Downloaded/Chakraborty, Allamanis, Ray - 2018 - Tree2Tree Neural Translation Model for Learning Source Code Changes.pdf:pdf},
mendeley-groups = {Code Analysis/Summarization},
pages = {1--12},
title = {{Tree2Tree Neural Translation Model for Learning Source Code Changes}},
url = {http://arxiv.org/abs/1810.00314},
year = {2018}
}
@article{Lu2019,
abstract = {The online programing services, such as Github,TopCoder, and EduCoder, have promoted a lot of social interactions among the service users. However, the existing social interactions is rather limited and inefficient due to the rapid increasing of source-code repositories, which is difficult to explore manually. The emergence of source-code mining provides a promising way to analyze those source codes, so that those source codes can be relatively easy to understand and share among those service users. Among all the source-code mining attempts,program classification lays a foundation for various tasks related to source-code understanding, because it is impossible for a machine to understand a computer program if it cannot classify the program correctly. Although numerous machine learning models, such as the Natural Language Processing (NLP) based models and the Abstract Syntax Tree (AST) based models, have been proposed to classify computer programs based on their corresponding source codes, the existing works cannot fully characterize the source codes from the perspective of both the syntax and semantic information. To address this problem, we proposed a Graph Neural Network (GNN) based model, which integrates data flow and function call information to the AST,and applies an improved GNN model to the integrated graph, so as to achieve the state-of-art program classification accuracy. The experiment results have shown that the proposed work can classify programs with accuracy over 97{\%}.},
annote = {Contribution:

Graph: AST, data flow, function call

Method:
Classification of programs apparently for recommendation.

Task: classify program with Gated GANN},
archivePrefix = {arXiv},
arxivId = {1903.03804},
author = {Lu, Mingming and Tan, Dingwu and Xiong, Naixue and Chen, Zailiang and Li, Haifeng},
eprint = {1903.03804},
file = {:Volumes/External/data{\_}sink/1903.03804.pdf:pdf},
mendeley-groups = {Graph as Representation/Code/Other Graph Approaches,Code Analysis/Classification,Code Analysis/Graphs},
number = {8},
pages = {1--12},
title = {{Program Classification Using Gated Graph Attention Neural Network for Online Programming Service}},
url = {http://arxiv.org/abs/1903.03804},
volume = {14},
year = {2019}
}
@inproceedings{Wei2020LambdaNetPT,
  title={LambdaNet: Probabilistic Type Inference using Graph Neural Networks},
  author={Jiayi Wei and M Goyal and Greg Durrett and Isil Dillig},
  booktitle={ICLR 2020},
  year={2020}
}
@article{Chen2019,
abstract = {Natural language processing has improved tremendously after the success of word embedding techniques such as word2vec. Recently, the same idea has been applied on source code with encouraging results. In this survey, we aim to collect and discuss the usage of word embedding techniques on programs and source code. The articles in this survey have been collected by asking authors of related work and with an extensive search on Google Scholar. Each article is categorized into five categories: 1. embedding of tokens 2. embedding of functions or methods 3. embedding of sequences or sets of method calls 4. embedding of binary code 5. other embeddings. We also provide links to experimental data and show some remarkable visualization of code embeddings. In summary, word embedding has been successfully applied on different granularities of source code. With access to countless open-source repositories, we see a great potential of applying other data-driven natural language processing techniques on source code in the future.},
annote = {Overviews several levels of embedding code embedding spaces from tokens to methods and sequence of function calls.


Token embeddings were used for vulnerability detection, automatic program repairing, building user model of students. Most likely those methods are interesting not because they directly applued of the NLP methods for the code, but due to the additional transformation steps they made. 


There were models that encode fucntion representation using some kind of language model. The ones that are known are LSTM, log-billinear, encoder-decoder architectures, poincare embeddings from function call graph, Devlin et al. (2017) use something similar to tree traversal to create an embeddings (one of th efew that is similar to what i want), siamese networks for identical methods detection (mayme they renamed variabes),


There are plenty embedding methods, each is created for a specific task

Check Structure2vec.},
archivePrefix = {arXiv},
arxivId = {1904.03061},
author = {Chen, Zimin and Monperrus, Martin},
eprint = {1904.03061},
file = {:Volumes/External/data{\_}sink/1904.03061.pdf:pdf},
mendeley-groups = {Graph as Representation/Code/Competitor,Code Analysis/Survey},
pages = {1--8},
title = {{A Literature Study of Embeddings on Source Code}},
url = {http://arxiv.org/abs/1904.03061},
year = {2019}
}

@inproceedings{Yahav2018,
  title={From programs to interpretable deep models and back},
  author={Yahav, Eran},
  booktitle={International Conference on Computer Aided Verification},
  pages={27--37},
  year={2018},
  organization={Springer}
}

@article{White2016,
abstract = {Code clone detection is an important problem for software maintenance and evolution. Many approaches consider either structure or identifiers, but none of the existing detection techniques model both sources of information. These techniques also depend on generic, handcrafted features to represent code fragments. We introduce learning-based detection techniques where everything for representing terms and fragments in source code is mined from the repository. Our code analysis supports a framework, which relies on deep learning, for automatically linking patterns mined at the lexical level with patterns mined at the syntactic level. We evaluated our novel learning-based approach for code clone detection with respect to feasibility from the point of view of software maintainers. We sampled and manually evaluated 398 fileand 480 method-level pairs across eight real-world Java systems; 93{\%} of the fileand method-level samples were evaluated to be true positives. Among the true positives, we found pairs mapping to all four clone types.We compared our approach to a traditional structure-oriented technique and found that our learning-based approach detected clones that were either undetected or suboptimally reported by the prominent tool Deckard. Our results affirm that our learning-based approach is suitable for clone detection and a tenable technique for researchers.},
annote = {Task: Clone detection

Approach: RNN?

Mention graph based approaches in citations},
author = {White, Martin and Tufano, Michele and Vendome, Christopher and Poshyvanyk, Denys},
doi = {10.1145/2970276.2970326},
file = {:home/ltv/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/White et al. - 2016 - Deep learning code fragments for code clone detection.pdf:pdf},
isbn = {9781450338455},
journal = {ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
keywords = {Abstract syntax trees,Code clone detection,Deep learning,Language models,Machine learning,Neu-ral networks},
mendeley-groups = {Code Analysis/Similarity Search},
pages = {87--98},
title = {{Deep learning code fragments for code clone detection}},
year = {2016}
}

@article{Allamanis2017a,
  title={A survey of machine learning for big code and naturalness},
  author={Allamanis, Miltiadis and Barr, Earl T and Devanbu, Premkumar and Sutton, Charles},
  journal={ACM Computing Surveys (CSUR)},
  volume={51},
  number={4},
  pages={1--37},
  year={2018},
  publisher={ACM New York, NY, USA}
}

